{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "moral-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "import MeCab\n",
    "import unidic\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "brilliant-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../data/all_entities.json.gz'\n",
    "with gzip.open(input_file, \"rt\", encoding=\"utf-8\") as fin:\n",
    "    lines = fin.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eight-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = dict()\n",
    "for line in lines:\n",
    "    entity = json.loads(line.strip())\n",
    "    entities[entity[\"title\"]] = entity[\"text\"]\n",
    "del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bronze-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger('-d \"{}\"'.format(unidic.DICDIR))\n",
    "STOP_POSTAGS = ('代名詞', '接続詞', '感動詞', '動詞,非自立可能', '助動詞', '助詞', '接頭辞', '記号,一般', '補助記号', '空白', 'BOS/EOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "raised-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text):\n",
    "    node = tagger.parseToNode(text)\n",
    "    tokens = []\n",
    "    while node:\n",
    "        if node.feature.startswith(STOP_POSTAGS):\n",
    "            pass\n",
    "        elif len(feature := node.feature.split(\",\")) > 7:            \n",
    "            tokens += [feature[7].lower()]\n",
    "        else:\n",
    "            tokens += [node.surface.lower()]\n",
    "        node = node.next\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bottom-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../ir_dumps/doc_id2title.pickle', 'rb') as f:\n",
    "    doc_id2title = pickle.load(f)\n",
    "with open('../ir_dumps/doc_id2token_count.pickle', 'rb') as f:\n",
    "    doc_id2token_count = pickle.load(f)\n",
    "with open('../ir_dumps/token2pointer.pickle', 'rb') as f:\n",
    "    token2pointer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "finite-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index_line(index_line):\n",
    "    return list(map(lambda x: tuple(map(int, x.split(':'))), index_line.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "opponent-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_entity_candidates(query, candidates, topk=10):\n",
    "    avgdl = sum(doc_id2token_count) / len(doc_id2token_count)\n",
    "    parsed_query = parse_text(query)\n",
    "    target_posting = {}\n",
    "    with open('../ir_dumps/inverted_index', 'r', encoding='utf-8') as index_file:\n",
    "        for token in parsed_query:\n",
    "            if token in token2pointer:\n",
    "                pointer, offset = token2pointer[token]\n",
    "                index_file.seek(pointer)\n",
    "                index_line = index_file.read(offset-pointer).rstrip()\n",
    "                postings_list = load_index_line(index_line)\n",
    "                target_posting[token] = postings_list\n",
    "\n",
    "    # bm25スコアでor検索\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    all_docs = len(entities)\n",
    "    doc_id2tfidf = [0 for i in range(all_docs)]\n",
    "    for token, postings_list in target_posting.items():\n",
    "        idf = math.log2((all_docs-len(postings_list)+0.5) / (len(postings_list) + 0.5))\n",
    "        # idfが負になる単語は一般的すぎるので無視\n",
    "        idf = max(idf, 0)\n",
    "        if idf == 0:\n",
    "            continue\n",
    "        for doc_id, tf in postings_list:\n",
    "            dl = doc_id2token_count[doc_id]\n",
    "            token_tfidf = idf * ((tf * (k1 + 1))/(tf + k1 * (1-b+b*(dl/avgdl))))\n",
    "            doc_id2tfidf[doc_id] += token_tfidf\n",
    "    \n",
    "    # candidateごとの検索\n",
    "    search_results = []\n",
    "    with open('../ir_dumps/inverted_index', 'r', encoding='utf-8') as index_file:\n",
    "        for candidate in candidates:\n",
    "            parsed_candidate = parse_text(candidate)\n",
    "            \n",
    "            candidate_target_posting = {}\n",
    "            for token in parsed_candidate:\n",
    "                if token in token2pointer:\n",
    "                    pointer, offset = token2pointer[token]\n",
    "                    index_file.seek(pointer)\n",
    "                    index_line = index_file.read(offset-pointer).rstrip()\n",
    "                    postings_list = load_index_line(index_line)\n",
    "                    candidate_target_posting[token] = postings_list\n",
    "                    \n",
    "            candidate_tfidf = []\n",
    "            # candidateとなる文字列が含まれるdoc_idの集合\n",
    "            candidate_doc_ids = set()\n",
    "            for token_position, (token, postings_list) in enumerate(candidate_target_posting.items()):\n",
    "                idf = math.log2((all_docs-len(postings_list)+0.5) / (len(postings_list) + 0.5))\n",
    "                # idfが負になる単語は一般的すぎるので無視\n",
    "                idf = max(idf, 0)\n",
    "                if idf == 0:\n",
    "                    continue\n",
    "                token_doc_ids = []\n",
    "                for doc_id, tf in postings_list:\n",
    "                    dl = doc_id2token_count[doc_id]\n",
    "                    token_tfidf = idf * ((tf * (k1 + 1))/(tf + k1 * (1-b+b*(dl/avgdl))))\n",
    "                    doc_id2tfidf[doc_id] += token_tfidf\n",
    "                    candidate_tfidf += [(doc_id, token_tfidf)]\n",
    "                    token_doc_ids += [doc_id]\n",
    "                \n",
    "                if token_position == 0:\n",
    "                    candidate_doc_ids |= set(token_doc_ids)\n",
    "                else:\n",
    "                    candidate_doc_ids &= set(token_doc_ids)\n",
    "\n",
    "            docs = [(doc_id, doc_id2tfidf[doc_id]) for doc_id in candidate_doc_ids]\n",
    "            docs = sorted(docs, key=lambda x: x[1], reverse=True)\n",
    "            search_results += [docs[:topk]]\n",
    "            for doc_id, tfidf in candidate_tfidf:\n",
    "                doc_id2tfidf[doc_id] -= tfidf\n",
    "            \n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "norwegian-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_argument_wrapper(args):\n",
    "    return search_entity_candidates(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "complex-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_list(search_result, topk=5):\n",
    "    sentence_list = []\n",
    "    for doc_id, _ in search_result[:topk]:\n",
    "        title = doc_id2title[doc_id]\n",
    "        text = entities[title]\n",
    "        sentences = text.split('。')\n",
    "        sentence_list += sentences\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "collect-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_sentence(query, candidate, sentence_list, topk=10):\n",
    "    inverted_index = defaultdict(list)\n",
    "    sentence_id2sentence = [sentence for sentence in sentence_list]\n",
    "    sentence_id2token_count = []\n",
    "#     for sentence_id, sentence in tqdm(enumerate(sentence_list), total=len(sentence_list)):\n",
    "    for sentence_id, sentence in enumerate(sentence_list):\n",
    "        tokens = parse_text(sentence)\n",
    "    \n",
    "        sentence_id2token_count += [len(tokens)]\n",
    "\n",
    "        count_tokens = Counter(tokens)\n",
    "        for token, count in count_tokens.items():\n",
    "            inverted_index[token] += [(sentence_id, count)]\n",
    "\n",
    "    avgdl = sum(sentence_id2token_count) / len(sentence_id2token_count)\n",
    "    parsed_query = parse_text(query)\n",
    "    parsed_query += parse_text(candidate)\n",
    "    target_posting = {}\n",
    "    for token in parsed_query:\n",
    "        if token in inverted_index:\n",
    "            postings_list = inverted_index[token]\n",
    "            target_posting[token] = postings_list\n",
    "\n",
    "    # bm25スコアでor検索\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    all_docs = len(sentence_list)\n",
    "    sentence_id2tfidf = [0 for i in range(all_docs)]\n",
    "    for token, postings_list in target_posting.items():\n",
    "        idf = math.log2((all_docs-len(postings_list)+0.5) / (len(postings_list) + 0.5))\n",
    "        # idfが負になる単語は一般的すぎるので無視\n",
    "        idf = max(idf, 0)\n",
    "        if idf == 0:\n",
    "            continue\n",
    "        for sentence_id, tf in postings_list:\n",
    "            dl = sentence_id2token_count[sentence_id]\n",
    "            token_tfidf = idf * ((tf * (k1 + 1))/(tf + k1 * (1-b+b*(dl/avgdl))))\n",
    "            sentence_id2tfidf[sentence_id] += token_tfidf\n",
    "\n",
    "    sentences = [(sentence_id, tfidf) for sentence_id, tfidf in enumerate(sentence_id2tfidf) if tfidf != 0]\n",
    "    sentences = sorted(sentences, key=lambda x: x[1], reverse=True)\n",
    "    return list(map(lambda x: (sentence_id2sentence[x[0]], x[1]), sentences[:topk]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "transparent-approval",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13061/13061 [00:00<00:00, 69757.30it/s]\n"
     ]
    }
   ],
   "source": [
    "input_file = '../data/train_questions.json'\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin:\n",
    "    lines = fin.readlines()\n",
    "\n",
    "queries = []\n",
    "candidates_list = []\n",
    "for line in tqdm(lines):\n",
    "    data_raw = json.loads(line.strip(\"\\n\"))\n",
    "#     qid = data_raw[\"qid\"]\n",
    "    question = data_raw[\"question\"].replace(\"_\", \"\")  # \"_\" は cloze question\n",
    "    options = data_raw[\"answer_candidates\"]  # TODO\n",
    "#     answer = data_raw[\"answer_entity\"]\n",
    "    queries += [question]\n",
    "    candidates_list += [options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "generic-internship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'格闘家ボブ・サップの出身国はどこでしょう?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "muslim-national",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['アメリカ合衆国',\n",
       " 'ミネソタ州',\n",
       " 'オンタリオ州',\n",
       " 'ペンシルベニア州',\n",
       " 'オレゴン州',\n",
       " 'ニューヨーク州',\n",
       " 'コロラド州',\n",
       " 'オーストラリア',\n",
       " 'ニュージャージー州',\n",
       " 'マサチューセッツ州',\n",
       " 'カナダ',\n",
       " 'テキサス州',\n",
       " 'ミシガン州',\n",
       " 'ワシントン州',\n",
       " 'ニュージーランド',\n",
       " 'オハイオ州',\n",
       " 'カリフォルニア州',\n",
       " 'メリーランド州',\n",
       " 'イリノイ州',\n",
       " 'イギリス']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "opponent-revolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.51 s, sys: 68.4 ms, total: 4.58 s\n",
      "Wall time: 4.58 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(503868, 67.16353159534238),\n",
       " (305429, 66.78403294179013),\n",
       " (795598, 66.12236396817468),\n",
       " (149041, 55.385281614879695),\n",
       " (457401, 53.10957954882017),\n",
       " (187616, 51.31076360804543),\n",
       " (85846, 47.76138663830434),\n",
       " (798722, 47.521333481150016),\n",
       " (838757, 45.60942607128479),\n",
       " (477688, 44.170923102934026)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "search_results = search_entity_candidates(queries[0], candidates_list[0])\n",
    "search_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "scientific-active",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('チャド・バノン', 'ボブ・サップ', 'マット・ヒューム')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_id2title[503868], doc_id2title[305429], doc_id2title[795598]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "healthy-stewart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(305429, 67.76079779388517),\n",
       " (715678, 47.623069924615784),\n",
       " (813895, 45.23169151314288),\n",
       " (746282, 44.59881819156162),\n",
       " (451862, 41.65596099849918),\n",
       " (449421, 40.626545292485474),\n",
       " (894796, 40.09513261039599),\n",
       " (851731, 39.178850788243054),\n",
       " (725307, 38.81948050223761),\n",
       " (404124, 37.660272326206965)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "roman-festival",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ミネソタ-minnesota', '州']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_text('ミネソタ州')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "uniform-poker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ミネソタ州' in entities['ボブ・サップ'], 'ミネソタ' in entities['ボブ・サップ'], '州' in entities['ボブ・サップ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "latter-stroke",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(512070, 42.23082025433194),\n",
       " (391473, 40.218478525205306),\n",
       " (11451, 38.814849054614676),\n",
       " (690730, 38.734221194828606),\n",
       " (614755, 38.689525015104074),\n",
       " (816922, 37.50988913182361),\n",
       " (149660, 36.74755099732139),\n",
       " (141661, 36.46235181669672),\n",
       " (165329, 36.2479389065969),\n",
       " (490924, 35.825664500682386)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "durable-contrary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ソロモン・ノーサップ'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_id2title[512070]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "special-mapping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'オンタリオ州' in entities['ソロモン・ノーサップ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "decimal-alexander",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [16:02<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.99 s, sys: 563 ms, total: 2.55 s\n",
      "Wall time: 16min 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with Pool(cpu_count()) as p:\n",
    "#     results = list(tqdm(p.imap(parse_argument_wrapper, zip(queries, candidates_list)), total=len(queries)))\n",
    "    partial_results = list(tqdm(p.imap(parse_argument_wrapper, zip(queries[:1000], candidates_list[:1000])), total=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "employed-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../ir_dumps/train_search_cadidates_results', 'wb') as f:\n",
    "#     pickle.dump(results, f)\n",
    "with open('../ir_dumps/partial_train_search_cadidates_results', 'wb') as f:\n",
    "    pickle.dump(partial_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "powerful-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 0\n",
    "result = search_entity_candidates(queries[sample_index], candidates_list[sample_index], topk=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "regional-repair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate=アメリカ合衆国\n",
      "[('チャド・バノン（Chad Bannon、1970年11月13日 - ）は、アメリカ合衆国の男性格闘家、俳優である', 26.582446203988724), ('マット・ヒューム（Matt Hume、1966年7月14日 - ）は、アメリカ合衆国の男性総合格闘家', 26.582446203988724), ('ボブ・サップ（Bob Sapp、1973年9月22日 - ）は、アメリカ合衆国のキックボクサー、プロレスラー、総合格闘家、タレント、俳優、元アメリカンフットボール選手', 26.156155272330924), ('彼とは別にアメリカ出身で同姓同名の格闘家・マイケル・マクドナルドが存在するが、こちらは1991年生まれでありUFCを中心として総合格闘技一本に絞った競技生活を送っている', 16.810192741467606), (' ^ Bob Sapp explains DREAM \"Dynamite!! 2010\" no-show, says DREAM is \"broke\" MMA JUNKIE.COM 2011年1月18日 ^ SimonInoki 2011年1月17日 ^ 野獣ボブ・サップ怒りの暴露「格闘技界のカネ、女、ＦＢＩ」 vol.1デジタル大衆 週刊大衆4月1日号 ^ 野獣ボブ・サップ怒りの暴露「格闘技界のカネ、女、ＦＢＩ」 vol.2 デジタル大衆 週刊大衆4月1日号 ^ カナダ出身の格闘家で、1965年生まれ', 10.589177345992997), ('なお、ウォーレン・サップはフロリダ州オーランド出身であり、マイアミ大学の卒業生である', 9.315335484332618), ('コロラド州出身', 8.504074349386341), ('コロラド州コロラドスプリングス出身', 8.161026252958882), ('ワシントン州シアトル出身', 8.161026252958882), (' 一部で「ボブ・サップは元NFLプレイヤーのウォーレン・サップ（2013年プロフットボール殿堂入り）の兄弟（または従兄弟）である」というまったくの誤報が流れたが、ボブ・サップとウォーレン・サップとの間に特筆すべき血縁関係はない', 7.961718535845536)]\n",
      "candidate=ミネソタ州\n",
      "[('ミネソタ州ミネアポリス出身', 25.17586063063079), ('ミネソタ州アイオタ出身', 24.209337902724197), ('ボブ・サップ（Bob Sapp、1973年9月22日 - ）は、アメリカ合衆国のキックボクサー、プロレスラー、総合格闘家、タレント、俳優、元アメリカンフットボール選手', 16.396103350378425), ('コロラド州コロラドスプリングス出身', 16.171896611826902), ('なお、ウォーレン・サップはフロリダ州オーランド出身であり、マイアミ大学の卒業生である', 15.843546996492666), ('ローガン・クラーク（Logan Clark、1985年2月16日 - ）は、アメリカ合衆国の男性総合格闘家', 14.126606192657787), ('ケリー・コボルド（Kelly Kobold、1983年2月25日 - ）は、アメリカ合衆国の女性総合格闘家', 13.384036092218903), ('彼とは別にアメリカ出身で同姓同名の格闘家・マイケル・マクドナルドが存在するが、こちらは1991年生まれでありUFCを中心として総合格闘技一本に絞った競技生活を送っている', 12.284744793940778), (' ^ Bob Sapp explains DREAM \"Dynamite!! 2010\" no-show, says DREAM is \"broke\" MMA JUNKIE.COM 2011年1月18日 ^ SimonInoki 2011年1月17日 ^ 野獣ボブ・サップ怒りの暴露「格闘技界のカネ、女、ＦＢＩ」 vol.1デジタル大衆 週刊大衆4月1日号 ^ 野獣ボブ・サップ怒りの暴露「格闘技界のカネ、女、ＦＢＩ」 vol.2 デジタル大衆 週刊大衆4月1日号 ^ カナダ出身の格闘家で、1965年生まれ', 10.790782220495231), ('大学で総合格闘技に触れ、学生とプロ総合格闘家の二足の草鞋を履いていた', 9.657763760653955)]\n",
      "candidate=オンタリオ州\n",
      "[('オンタリオ州トロント出身', 25.74469481393327), ('アイヴァン・サラベリー（Ivan Salaverry、1971年1月11日 - ）は、カナダの男性総合格闘家', 13.86497768850895), (' シカゴ・サンタイムスは、カナダオンタリオ州トロントのホリンジャー社(Hollinger Inc.)に所有されており、このグループはカナダ人実業家のコンラッド・ブラックに経営管理されている', 13.652130469396134), (' また、ボブ・グリーンがこの新聞で記者生活を始めた', 12.256898607584453), ('アメリカ合衆国ワシントン州シアトル在住', 11.396385840944642), ('ノーサップのオンタリオ州ストリーツヴィル（英語版）訪問は広く報道されたが、反感を持ったカナダ人群衆のせいで講演は中止された', 11.319027410068117), (' 新聞 アメリカ合衆国 - イリノイ州 - シカゴ Chicago Sun-Times（英語版）', 9.836891464442358), (' 無罪判決が下った後、バーチはノーサップを相手取り、不当にジョージア州出身の奴隷と偽って買い取り代金625ドルを騙し取ったとして訴えを起こした', 9.242283307257468), ('シカゴ・サンタイムズ（Chicago Sun-Times）は、アメリカ合衆国イリノイ州シカゴ市で発行されている新聞である', 9.090517429287956), ('バーチはまた、ノーサップをジョージア州からの奴隷と偽り、ノーサップはアメリカ合衆国議会議事堂近くにあった、ウィリアム・ウィリアムズ（英: William Williams）へ出品された', 8.827823210778531)]\n"
     ]
    }
   ],
   "source": [
    "for candidate_index in range(3):\n",
    "    candidate = candidates_list[sample_index][candidate_index]\n",
    "    print(f'candidate={candidate}')\n",
    "    sentence_list = get_sentence_list(result[candidate_index])\n",
    "    sorted_sentence = sort_sentence(queries[0], candidates_list[0][candidate_index], sentence_list)\n",
    "    print(sorted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-worse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
