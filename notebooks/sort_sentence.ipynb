{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unexpected-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "import MeCab\n",
    "import unidic\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lonely-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index_line(index_line):\n",
    "    return list(map(lambda x: tuple(map(int, x.split(':'))), index_line.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "refined-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def search_entity(query, topk=13):\n",
    "def search_entity(query, topk=10):\n",
    "    avgdl = sum(doc_id2token_count) / len(doc_id2token_count)\n",
    "    parsed_query = parse_text(query)\n",
    "    target_posting = {}\n",
    "    with open('../ir_dump/inverted_index', 'r', encoding='utf-8') as index_file:\n",
    "        for token in parsed_query:\n",
    "            if token in token2pointer:\n",
    "                pointer, offset = token2pointer[token]\n",
    "                index_file.seek(pointer)\n",
    "                index_line = index_file.read(offset-pointer).rstrip()\n",
    "                postings_list = load_index_line(index_line)\n",
    "                target_posting[token] = postings_list\n",
    "\n",
    "    # bm25スコアでor検索\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    all_docs = len(entities)\n",
    "    doc_id2tfidf = [0 for i in range(all_docs)]\n",
    "    for token, postings_list in target_posting.items():\n",
    "        idf = math.log2((all_docs-len(postings_list)+0.5) / (len(postings_list) + 0.5))\n",
    "        # idfが負になる単語は一般的すぎるので無視\n",
    "        idf = max(idf, 0)\n",
    "        if idf == 0:\n",
    "            continue\n",
    "        for doc_id, tf in postings_list:\n",
    "            dl = doc_id2token_count[doc_id]\n",
    "            token_tfidf = idf * ((tf * (k1 + 1))/(tf + k1 * (1-b+b*(dl/avgdl))))\n",
    "            doc_id2tfidf[doc_id] += token_tfidf\n",
    "\n",
    "    docs = [(doc_id, tfidf) for doc_id, tfidf in enumerate(doc_id2tfidf) if tfidf != 0]\n",
    "    docs = sorted(docs, key=lambda x: x[1], reverse=True)\n",
    "    return docs[:topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assured-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_list(search_result, topk=5):\n",
    "    sentence_list = []\n",
    "    for doc_id, _ in search_result[:topk]:\n",
    "        title = doc_id2title[doc_id]\n",
    "        text = entities[title]\n",
    "        sentences = text.split('。')\n",
    "        sentence_list += sentences\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "clean-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../data/all_entities.json.gz'\n",
    "with gzip.open(input_file, \"rt\", encoding=\"utf-8\") as fin:\n",
    "    lines = fin.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reserved-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = dict()\n",
    "for line in lines:\n",
    "    entity = json.loads(line.strip())\n",
    "    entities[entity[\"title\"]] = entity[\"text\"]\n",
    "del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "previous-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger('-d \"{}\"'.format(unidic.DICDIR))\n",
    "STOP_POSTAGS = ('代名詞', '接続詞', '感動詞', '動詞,非自立可能', '助動詞', '助詞', '接頭辞', '記号,一般', '補助記号', '空白', 'BOS/EOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "african-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text):\n",
    "    node = tagger.parseToNode(text)\n",
    "    tokens = []\n",
    "    while node:\n",
    "        if node.feature.startswith(STOP_POSTAGS):\n",
    "            pass\n",
    "        elif len(feature := node.feature.split(\",\")) > 7:            \n",
    "            tokens += [feature[7].lower()]\n",
    "        else:\n",
    "            tokens += [node.surface.lower()]\n",
    "        node = node.next\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mysterious-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_sentence(query, sentence_list, topk=10):\n",
    "    inverted_index = defaultdict(list)\n",
    "    sentence_id2sentence = [sentence for sentence in sentence_list]\n",
    "    sentence_id2token_count = []\n",
    "#     for sentence_id, sentence in tqdm(enumerate(sentence_list), total=len(sentence_list)):\n",
    "    for sentence_id, sentence in enumerate(sentence_list):\n",
    "        tokens = parse_text(sentence)\n",
    "    \n",
    "        sentence_id2token_count += [len(tokens)]\n",
    "\n",
    "        count_tokens = Counter(tokens)\n",
    "        for token, count in count_tokens.items():\n",
    "            inverted_index[token] += [(sentence_id, count)]\n",
    "\n",
    "    avgdl = sum(sentence_id2token_count) / len(sentence_id2token_count)\n",
    "    parsed_query = parse_text(query)\n",
    "    target_posting = {}\n",
    "    for token in parsed_query:\n",
    "        if token in inverted_index:\n",
    "            postings_list = inverted_index[token]\n",
    "            target_posting[token] = postings_list\n",
    "\n",
    "    # bm25スコアでor検索\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    all_docs = len(sentence_list)\n",
    "    sentence_id2tfidf = [0 for i in range(all_docs)]\n",
    "    for token, postings_list in target_posting.items():\n",
    "        idf = math.log2((all_docs-len(postings_list)+0.5) / (len(postings_list) + 0.5))\n",
    "        # idfが負になる単語は一般的すぎるので無視\n",
    "        idf = max(idf, 0)\n",
    "        if idf == 0:\n",
    "            continue\n",
    "        for sentence_id, tf in postings_list:\n",
    "            dl = sentence_id2token_count[sentence_id]\n",
    "            token_tfidf = idf * ((tf * (k1 + 1))/(tf + k1 * (1-b+b*(dl/avgdl))))\n",
    "            sentence_id2tfidf[sentence_id] += token_tfidf\n",
    "\n",
    "    sentences = [(sentence_id, tfidf) for sentence_id, tfidf in enumerate(sentence_id2tfidf) if tfidf != 0]\n",
    "    sentences = sorted(sentences, key=lambda x: x[1], reverse=True)\n",
    "    return list(map(lambda x: (sentence_id2sentence[x[0]], x[1]), sentences[:topk]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "phantom-thompson",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13061/13061 [00:00<00:00, 119165.75it/s]\n"
     ]
    }
   ],
   "source": [
    "input_file = '../data/train_questions.json'\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin:\n",
    "    lines = fin.readlines()\n",
    "\n",
    "queries = []\n",
    "for line in tqdm(lines):\n",
    "    data_raw = json.loads(line.strip(\"\\n\"))\n",
    "#     qid = data_raw[\"qid\"]\n",
    "    question = data_raw[\"question\"].replace(\"_\", \"\")  # \"_\" は cloze question\n",
    "#     options = data_raw[\"answer_candidates\"]  # TODO\n",
    "#     answer = data_raw[\"answer_entity\"]\n",
    "    queries += [question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "objective-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../ir_dump/doc_id2title.pickle', 'rb') as f:\n",
    "    doc_id2title = pickle.load(f)\n",
    "with open('../ir_dump/doc_id2token_count.pickle', 'rb') as f:\n",
    "    doc_id2token_count = pickle.load(f)\n",
    "with open('../ir_dump/token2pointer.pickle', 'rb') as f:\n",
    "    token2pointer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "seven-girlfriend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.01 s, sys: 20 ms, total: 1.03 s\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = search_entity(queries[0], topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "destroyed-structure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.3 ms, sys: 3.96 ms, total: 31.2 ms\n",
      "Wall time: 31.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sentence_list = entities[doc_id2title[result[0][0]]].split('。')\n",
    "sentence_list = get_sentence_list(result)\n",
    "sorted_sentence = sort_sentence(queries[0], sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "proved-hawaii",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ボブ・サップ（Bob Sapp、1973年9月22日 - ）は、アメリカ合衆国のキックボクサー、プロレスラー、総合格闘家、タレント、俳優、元アメリカンフットボール選手',\n",
       "  18.001643434637835),\n",
       " ('彼とは別にアメリカ出身で同姓同名の格闘家・マイケル・マクドナルドが存在するが、こちらは1991年生まれでありUFCを中心として総合格闘技一本に絞った競技生活を送っている',\n",
       "  13.911810695403958),\n",
       " (' ^ Bob Sapp explains DREAM \"Dynamite!! 2010\" no-show, says DREAM is \"broke\" MMA JUNKIE.COM 2011年1月18日 ^ SimonInoki 2011年1月17日 ^ 野獣ボブ・サップ怒りの暴露「格闘技界のカネ、女、ＦＢＩ」 vol.1デジタル大衆 週刊大衆4月1日号 ^ 野獣ボブ・サップ怒りの暴露「格闘技界のカネ、女、ＦＢＩ」 vol.2 デジタル大衆 週刊大衆4月1日号 ^ カナダ出身の格闘家で、1965年生まれ',\n",
       "  11.611478655388193),\n",
       " ('なお、ウォーレン・サップはフロリダ州オーランド出身であり、マイアミ大学の卒業生である', 9.802641668346661),\n",
       " (' 2010年、格闘家として第一線を退き、韓国でトークショーの司会や、会社の経営に乗り出していると明言', 9.05018428626646),\n",
       " ('コロラド州コロラドスプリングス出身', 8.847691695396007),\n",
       " (' 2002年、同じく元WCWの選手で友人のサム・グレコの紹介でK-1にスカウトされ、格闘家としてPRIDE、K-1に参戦',\n",
       "  8.163112398717395),\n",
       " (\" 男子総合格闘家一覧 男子キックボクサー一覧 プロレスラー一覧 DREAM選手一覧 PRIDE選手一覧 HERO'S選手一覧 K-1選手一覧 ニュースタッフプロダクション ボブ・サップ オフィシャルサイト ボブ・サップ オフィシャルブログ Bob Sapp ボブ サップ (@bobsappmma) - Twitter ボブ サップ - Facebook BobSappTV's channel - 公式YouTubeチャンネル ボブサップ日本語公式チャンネル ボブサップTV - 公式YouTubeチャンネル RIZIN 選手データ K-1sport.de - Complete Fighters Profile of Bob Sapp K-1 選手データ - ウェイバックマシン（2011年3月17日アーカイブ分） HERO'S 選手データ DREAM 選手データ バウトレビュー 選手データ ボブ・サップ - SHERDOGのプロフィール （英語） 通算成績と情報 Pro-Football-Reference, or\\xa0DatabaseFootball （英語）\",\n",
       "  8.076431321797525),\n",
       " (' 一部で「ボブ・サップは元NFLプレイヤーのウォーレン・サップ（2013年プロフットボール殿堂入り）の兄弟（または従兄弟）である」というまったくの誤報が流れたが、ボブ・サップとウォーレン・サップとの間に特筆すべき血縁関係はない',\n",
       "  7.88050091691942),\n",
       " ('格闘家として活躍時の体脂肪率は11 - 13%程度で、筋力維持のため毎日400g程度のプロテインパウダーを摂取しトレーニングに励んでいた',\n",
       "  7.6041131461861475)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-married",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
