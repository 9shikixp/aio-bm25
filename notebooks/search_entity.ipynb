{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "attractive-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "import MeCab\n",
    "import unidic\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "responsible-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../data/all_entities.json.gz'\n",
    "with gzip.open(input_file, \"rt\", encoding=\"utf-8\") as fin:\n",
    "    lines = fin.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "signed-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = dict()\n",
    "for line in lines:\n",
    "    entity = json.loads(line.strip())\n",
    "    entities[entity[\"title\"]] = entity[\"text\"]\n",
    "del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "innovative-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger('-d \"{}\"'.format(unidic.DICDIR))\n",
    "STOP_POSTAGS = ('代名詞', '接続詞', '感動詞', '動詞,非自立可能', '助動詞', '助詞', '接頭辞', '記号,一般', '補助記号', '空白', 'BOS/EOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "earlier-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text):\n",
    "    node = tagger.parseToNode(text)\n",
    "    tokens = []\n",
    "    while node:\n",
    "        if node.feature.startswith(STOP_POSTAGS):\n",
    "            pass\n",
    "        elif len(feature := node.feature.split(\",\")) > 7:            \n",
    "            tokens += [feature[7].lower()]\n",
    "        else:\n",
    "            tokens += [node.surface.lower()]\n",
    "        node = node.next\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "latest-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../ir_dump/doc_id2title.pickle', 'rb') as f:\n",
    "    doc_id2title = pickle.load(f)\n",
    "with open('../ir_dump/doc_id2token_count.pickle', 'rb') as f:\n",
    "    doc_id2token_count = pickle.load(f)\n",
    "with open('../ir_dump/token2pointer.pickle', 'rb') as f:\n",
    "    token2pointer = pickle.load(f)\n",
    "title2doc_id = {title:doc_id for doc_id, title in enumerate(doc_id2title)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "genuine-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index_line(index_line):\n",
    "    return list(map(lambda x: tuple(map(int, x.split(':'))), index_line.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "willing-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_entity(query, exclude_candidates=[], topk=10):\n",
    "    avgdl = sum(doc_id2token_count) / len(doc_id2token_count)\n",
    "    parsed_query = parse_text(query)\n",
    "    target_posting = {}\n",
    "    with open('../ir_dump/inverted_index', 'r', encoding='utf-8') as index_file:\n",
    "        for token in parsed_query:\n",
    "            if token in token2pointer:\n",
    "                pointer, offset = token2pointer[token]\n",
    "                index_file.seek(pointer)\n",
    "                index_line = index_file.read(offset-pointer).rstrip()\n",
    "                postings_list = load_index_line(index_line)\n",
    "                target_posting[token] = postings_list\n",
    "\n",
    "    # bm25スコアでor検索\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    all_docs = len(entities)\n",
    "    doc_id2tfidf = [0 for i in range(all_docs)]\n",
    "    for token, postings_list in target_posting.items():\n",
    "        idf = math.log2((all_docs-len(postings_list)+0.5) / (len(postings_list) + 0.5))\n",
    "        # idfが負になる単語は一般的すぎるので無視\n",
    "        idf = max(idf, 0)\n",
    "        if idf == 0:\n",
    "            continue\n",
    "        for doc_id, tf in postings_list:\n",
    "            dl = doc_id2token_count[doc_id]\n",
    "            token_tfidf = idf * ((tf * (k1 + 1))/(tf + k1 * (1-b+b*(dl/avgdl))))\n",
    "            doc_id2tfidf[doc_id] += token_tfidf\n",
    "    \n",
    "    # 選択肢を検索結果から除外\n",
    "    exclude_doc_ids = [title2doc_id[candidate] for candidate in exclude_candidates]\n",
    "    for exclude_doc_id in exclude_doc_ids:\n",
    "        doc_id2tfidf[exclude_doc_id] = 0\n",
    "    \n",
    "    docs = [(doc_id, tfidf) for doc_id, tfidf in enumerate(doc_id2tfidf) if tfidf != 0]\n",
    "    docs = sorted(docs, key=lambda x: x[1], reverse=True)\n",
    "    return docs[:topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reverse-mathematics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13061/13061 [00:00<00:00, 62630.90it/s]\n"
     ]
    }
   ],
   "source": [
    "input_file = '../data/train_questions.json'\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin:\n",
    "    lines = fin.readlines()\n",
    "\n",
    "queries = []\n",
    "candidates_list = []\n",
    "for line in tqdm(lines):\n",
    "    data_raw = json.loads(line.strip(\"\\n\"))\n",
    "#     qid = data_raw[\"qid\"]\n",
    "    question = data_raw[\"question\"].replace(\"_\", \"\")  # \"_\" は cloze question\n",
    "    options = data_raw[\"answer_candidates\"]  # TODO\n",
    "#     answer = data_raw[\"answer_entity\"]\n",
    "    queries += [question]\n",
    "    candidates_list += [options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rubber-service",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [07:29<00:00,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.94 s, sys: 519 ms, total: 2.46 s\n",
      "Wall time: 7min 29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with Pool(cpu_count()) as p:\n",
    "#     results = list(tqdm(p.imap(search_entity, queries), total=len(queries)))\n",
    "    partial_results = list(tqdm(p.imap(search_entity, queries[:1000]), total=len(queries[:1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "passing-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../ir_dump/train_search_results', 'wb') as f:\n",
    "with open('../ir_dump/partial_train_search_results', 'wb') as f:\n",
    "    pickle.dump(partial_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "trained-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_entity_exclude_candidates(args):\n",
    "    return search_entity(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "military-miller",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [07:24<00:00,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.21 s, sys: 558 ms, total: 2.77 s\n",
      "Wall time: 7min 24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with Pool(cpu_count()) as p:\n",
    "#     results = list(tqdm(p.imap(search_entity_exclude_candidates, zip(queries, candidates_list)), total=len(queries)))\n",
    "    partial_results = list(tqdm(p.imap(search_entity_exclude_candidates, zip(queries[:1000], candidates_list[:1000])), total=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "incomplete-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../ir_dump/train_search_exclude_candidates_results', 'wb') as f:\n",
    "with open('../ir_dump/partial_train_search_exclude_candidates_results', 'wb') as f:\n",
    "    pickle.dump(partial_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-processing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
