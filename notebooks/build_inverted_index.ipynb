{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "another-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "import MeCab\n",
    "import unidic\n",
    "\n",
    "from contextlib import ExitStack\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-prediction",
   "metadata": {},
   "source": [
    "# wikipedia読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "desirable-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../data/all_entities.json.gz'\n",
    "with gzip.open(input_file, \"rt\", encoding=\"utf-8\") as fin:\n",
    "    lines = fin.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prime-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = dict()\n",
    "for line in lines:\n",
    "    entity = json.loads(line.strip())\n",
    "    entities[entity[\"title\"]] = entity[\"text\"]\n",
    "del lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-instrumentation",
   "metadata": {},
   "source": [
    "# 形態素解析器初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedicated-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger('-d \"{}\"'.format(unidic.DICDIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "happy-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_POSTAGS = ('代名詞', '接続詞', '感動詞', '動詞,非自立可能', '助動詞', '助詞', '接頭辞', '記号,一般', '補助記号', '空白', 'BOS/EOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "processed-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text):\n",
    "    node = tagger.parseToNode(text)\n",
    "    tokens = []\n",
    "    while node:\n",
    "        if node.feature.startswith(STOP_POSTAGS):\n",
    "            pass\n",
    "        elif len(feature := node.feature.split(\",\")) > 7:            \n",
    "            tokens += [feature[7].lower()]\n",
    "        else:\n",
    "            tokens += [node.surface.lower()]\n",
    "        node = node.next\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-mapping",
   "metadata": {},
   "source": [
    "# 転置インデックス分割作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "regional-investor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 920172/920172 [56:36<00:00, 270.89it/s]  \n"
     ]
    }
   ],
   "source": [
    "partial_size = 10**5\n",
    "inverted_index = defaultdict(list)\n",
    "doc_id2title = []\n",
    "doc_id2token_count = []\n",
    "for doc_id, (title, text) in tqdm(enumerate(entities.items()), total=len(entities)):\n",
    "    tokens = parse_text(text)\n",
    "    \n",
    "    doc_id2title += [title]\n",
    "    doc_id2token_count += [len(tokens)]\n",
    "    \n",
    "    count_tokens = Counter(tokens)\n",
    "    for token, count in count_tokens.items():\n",
    "        inverted_index[token] += [(doc_id, count)]\n",
    "        \n",
    "    if (doc_id + 1) % partial_size == 0:\n",
    "        sorted_vocab = sorted(inverted_index.keys())\n",
    "        partial_id = doc_id // partial_size\n",
    "        \n",
    "        with open('../ir_dumps/partial_dict_{}'.format(partial_id), 'w', encoding='utf-8') as fout:\n",
    "            for token in sorted_vocab:\n",
    "                fout.write(token + '\\n')\n",
    "                \n",
    "        with open('../ir_dumps/partial_inverted_index_{}'.format(partial_id), 'w', encoding='utf-8') as fout:\n",
    "            for token in sorted_vocab:\n",
    "                posting_list = ' '.join([str(doc_id)+':'+str(tf)for doc_id, tf in inverted_index[token]])\n",
    "                fout.write(posting_list + '\\n')\n",
    "        inverted_index = defaultdict(list)\n",
    "\n",
    "\n",
    "sorted_vocab = sorted(inverted_index.keys())\n",
    "partial_id = (len(entities)-1) // partial_size\n",
    "\n",
    "with open('../ir_dumps/partial_dict_{}'.format(partial_id), 'w', encoding='utf-8') as fout:\n",
    "    for token in sorted_vocab:\n",
    "        fout.write(token + '\\n')\n",
    "\n",
    "with open('../ir_dumps/partial_inverted_index_{}'.format(partial_id), 'w', encoding='utf-8') as fout:\n",
    "    for token in sorted_vocab:\n",
    "        posting_list = ' '.join([str(doc_id)+':'+str(tf)for doc_id, tf in inverted_index[token]])\n",
    "        fout.write(posting_list + '\\n')\n",
    "\n",
    "# docment_idをタイトルやトークン数に変換するlistを保存\n",
    "with open('../ir_dumps/doc_id2title.pickle', 'wb') as f:\n",
    "    pickle.dump(doc_id2title, f)\n",
    "\n",
    "with open('../ir_dumps/doc_id2token_count.pickle', 'wb') as f:\n",
    "    pickle.dump(doc_id2token_count, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-privilege",
   "metadata": {},
   "source": [
    "# 分割転置インデックスのマージ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "qualified-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index_line(index_line):\n",
    "    if index_line == '':\n",
    "        return []\n",
    "    return list(map(lambda x: tuple(map(int, x.split(':'))), index_line.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mental-group",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 24013/3302574 [00:00<00:27, 120306.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329.8755843639374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3302574/3302574 [00:29<00:00, 111312.89it/s]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "dict_filenames = ['../ir_dumps/partial_dict_{}'.format(partial_id) for partial_id in range(10)]\n",
    "index_filenames = ['../ir_dumps/partial_inverted_index_{}'.format(partial_id) for partial_id in range(10)]\n",
    "line2token = []\n",
    "\n",
    "with ExitStack() as stack, open('../ir_dumps/inverted_index', 'w', encoding='utf-8') as fout:\n",
    "    dict_files = [stack.enter_context(open(fname, 'r', encoding='utf-8')) for fname in dict_filenames]\n",
    "    index_files = [stack.enter_context(open(fname, 'r', encoding='utf-8')) for fname in index_filenames]\n",
    "    tokens = []\n",
    "    postings = []\n",
    "    for dict_file, index_file in zip(dict_files, index_files):\n",
    "        token = dict_file.readline().rstrip()\n",
    "        index_line = index_file.readline().rstrip()\n",
    "        partial_posting_list = load_index_line(index_line)\n",
    "        tokens += [token]\n",
    "        postings += [partial_posting_list]\n",
    "        \n",
    "    while sorted_token := sorted(filter(lambda x: x != '', tokens)):\n",
    "        top_token = sorted_token[0]\n",
    "        posting_list = []\n",
    "        for partial_id, (dict_file, index_file) in enumerate(zip(dict_files, index_files)):\n",
    "            token = tokens[partial_id]\n",
    "            if token == top_token:\n",
    "                posting_list += postings[partial_id]\n",
    "                token = dict_file.readline().rstrip()\n",
    "                index_line = index_file.readline().rstrip()\n",
    "                partial_posting_list = load_index_line(index_line)\n",
    "                tokens[partial_id] = token\n",
    "                postings[partial_id] = partial_posting_list\n",
    "        \n",
    "        posting_list = ' '.join([str(doc_id)+':'+str(tf)for doc_id, tf in posting_list])\n",
    "        line2token += [top_token]\n",
    "        fout.write(posting_list + '\\n')\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print(end_time)\n",
    "\n",
    "# トークンと転置インデックスファイルのポインタ対応づけ\n",
    "token2pointer = {}\n",
    "with open('../ir_dumps/inverted_index', 'r', encoding='utf-8') as fin:\n",
    "    for token in tqdm(line2token):\n",
    "        start = fin.tell()\n",
    "        line = fin.readline()\n",
    "        end = fin.tell()\n",
    "        token2pointer[token] = (start, end)\n",
    "        \n",
    "with open('../ir_dumps/token2pointer.pickle', 'wb') as f:\n",
    "    pickle.dump(token2pointer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-choir",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
